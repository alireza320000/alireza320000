#imprt libarary
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.datasets import make_circles
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import Callback
from tensorflow.keras import initializers
import tensorflow as tf
from tensorflow import keras
#===========================================
#To illustrate the problem of gradient disappearance,
#let's try an example. The neural network is a nonlinear function.
#It should therefore be most appropriate for classifying nonlinear datasets.
#We use the make_circle () function of the scikit-learn function to generate some data
#This is not difficult to classify. One naive way is to build a
#3-tier neural network that can have great results:
#==============================
#This is not difficult to classify. One naive way is to build
#a 3-tier neural network that can have great results:
#=====================================================
# Make data: Two circles on x-y plane as a classification problem
X, y = make_circles(n_samples=1000,factor=0.5,noise=0.1)
plt.figure(figsize=(8, 4))
plt.scatter(X[:, 0], X[:, 1],cmap=plt.cm.coolwarm,c=y)
plt.show()
#=====================================================================
model = Sequential([
    Input(shape=(2,)),
    Dense(5, "relu"),
    Dense(1, "softmax")
])
model.compile(optimizer="RMSprop", loss="binary_crossentropy",metrics=["accuracy"])
model.fit(X,y, batch_size=64,epochs=200, verbose=True)
#=======================================================================================
print(model.evaluate(X,y))
#====================================
#Time is much worse. It seems to be worse by adding more layers (at least in my experiment):

model = Sequential([
    Input(shape=(2,)),
    Dense(5, "softmax"),
    Dense(5, "softmax"),
    Dense(5, "softmax"),
    Dense(1, "softmax")
])

model.compile(optimizer="RMSprop", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(X, y, batch_size=32, epochs=200, verbose=True)
#======================================================================================
print(model.evaluate(X, y))
#================================
class WeightCapture(Callback):
    "Capture the weights of each layer of the model"
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.weights = []
        self.epochs = []
 
    def on_epoch_end(self, epoch, logs=None):
        self.epochs.append(epoch) # remember the epoch axis
        weight = {}
        for layer in model.layers:
            if not layer.weights:
                continue
            name = layer.weights[0].name.split("/")[0]
            weight[name] = layer.weights[0].numpy()
        self.weights.append(weight)  
#==========================================================================
#Extract the class callback and define the on_epoch_end () function.
#This class needs the created model for initialization.
#At the end of each session, it reads each layer and stores the weights in the numpy array.
#=============================================================================================
def make_mlp(activation, initializer, name):
    "Create a model with specified activation and initalizer"
    model = Sequential([
        Input(shape=(2,), name=name+"0"),
        Dense(5, activation=activation, kernel_initializer=initializer, name=name+"1"),
        Dense(5, activation=activation, kernel_initializer=initializer, name=name+"2"),
        Dense(5, activation=activation, kernel_initializer=initializer, name=name+"3"),
        Dense(5, activation=activation, kernel_initializer=initializer, name=name+"4"),
        Dense(1, activation="sigmoid", kernel_initializer=initializer, name=name+"5")
    ])
    return model
#=================================================================================================
#We deliberately create a neural network with 4 hidden layers so we can see how each layer responds to the training.
#We will change the activation function of each hidden layer as well as the initial amount of weight.
#For ease of reference, we name each layer instead of allowing Keras to assign a name.
#The input is a coordinate on the xy plane, so the input shape of the vector is 2. The output is binary classification.
#So we use sigmoid activation to set the output in the range 0 to 1
#====================================================================================
initializer = tf.keras.initializers.RandomNormal=(mean=0.0, stddev=1.0)

batch_size = 32
n_epochs = 100
 
model = make_mlp("sigmoid", initializer, "sigmoid")
capture_cb = WeightCapture(model)
capture_cb.on_epoch_end(-1)
model.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=["acc"])
model.fit(X, y, batch_size=batch_size, epochs=n_epochs, callbacks=[capture_cb], verbose=1)
 

#To facilitate testing of different methods of generating an MLP, we create an auxiliary function to set up the neural network model:
#=========================================
print(model.evaluate(X, y))
#===========================
def plotweight(capture_cb):
    """Docstring: plot the weights mean and s.d across
    epochs
    """
    fig, ax = plt.subplots(2, 1, sharex=True, constrained_layout=True, figsize=(8, 10))
    for key in capture_cb.weights[0]:
        ax[0].plot(capture_cb.epochs, [w[key].mean() for w in capture_cb.weights], label=key)
    ax[0].legend()
    ax[1].set_title("S.D.")
    for key in capture_cb.weights[0]:
        ax[1].plot(capture_cb.epochs, [w[key].std() for w in capture_cb.weights], label=key)
    ax[1].legend()
    plt.show()
    
plotweight(capture_cb)
#============================================================================================
"""Docstring:tanh acitivation large varince gussian initlizer,
tnah
"""
capture_cb = WeightCapture(model)
capture_cb.on_epoch_end(-1)
model.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=["acc"])
model.fit(X, y, batch_size=batch_size, epochs=n_epochs, callbacks=[capture_cb], verbose=0)
print(model.evaluate(X,y))
plotweight(capture_cb)
#=====================================================================================================
# relu activation, large variance gaussian initialization
model = make_mlp("relu", initializer, "relu")
capture_cb = WeightCapture(model)
capture_cb.on_epoch_end(-1)
model.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=["acc"])
model.fit(X, y, batch_size=batch_size, epochs=n_epochs, callbacks=[capture_cb], verbose=0)
print(model.evaluate(X,y))
plotweight(capture_cb)
